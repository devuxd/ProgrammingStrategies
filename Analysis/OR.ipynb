{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of logistic ordinal regression (aka proportional odds) model\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy import linalg, optimize, sparse\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "BIG = 1e10\n",
    "SMALL = 1e-12\n",
    "\n",
    "\n",
    "def phi(t):\n",
    "    \"\"\"\n",
    "    logistic function, returns 1 / (1 + exp(-t))\n",
    "    \"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.empty(t.size, dtype=np.float)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "def log_logistic(t):\n",
    "    \"\"\"\n",
    "    (minus) logistic loss function, returns log(1 / (1 + exp(-t)))\n",
    "    \"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.zeros_like(t)\n",
    "    out[idx] = np.log(1 + np.exp(-t[idx]))\n",
    "    out[~idx] = (-t[~idx] + np.log(1 + np.exp(t[~idx])))\n",
    "    return out\n",
    "\n",
    "\n",
    "def ordinal_logistic_fit(X, y, alpha=0, l1_ratio=0, n_class=None, max_iter=10000,\n",
    "                         verbose=False, solver='TNC', w0=None):\n",
    "    \"\"\"\n",
    "    Ordinal logistic regression or proportional odds model.\n",
    "    Uses scipy's optimize.fmin_slsqp solver.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array, sparse matrix}, shape (n_samples, n_feaures)\n",
    "        Input data\n",
    "    y : array-like\n",
    "        Target values\n",
    "    max_iter : int\n",
    "        Maximum number of iterations\n",
    "    verbose: bool\n",
    "        Print convergence information\n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape (n_features,)\n",
    "        coefficients of the linear model\n",
    "    theta : array, shape (k,), where k is the different values of y\n",
    "        vector of thresholds\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    w0 = None\n",
    "\n",
    "    if not X.shape[0] == y.shape[0]:\n",
    "        raise ValueError('Wrong shape for X and y')\n",
    "\n",
    "    # .. order input ..\n",
    "    idx = np.argsort(y)\n",
    "    idx_inv = np.zeros_like(idx)\n",
    "    idx_inv[idx] = np.arange(idx.size)\n",
    "    X = X[idx]\n",
    "    y = y[idx].astype(np.int)\n",
    "    # make them continuous and start at zero\n",
    "    unique_y = np.unique(y)\n",
    "    for i, u in enumerate(unique_y):\n",
    "        y[y == u] = i\n",
    "    unique_y = np.unique(y)\n",
    "\n",
    "    # .. utility arrays used in f_grad ..\n",
    "    alpha = 0.\n",
    "    k1 = np.sum(y == unique_y[0])\n",
    "    E0 = (y[:, np.newaxis] == np.unique(y)).astype(np.int)\n",
    "    E1 = np.roll(E0, -1, axis=-1)\n",
    "    E1[:, -1] = 0.\n",
    "    E0, E1 = map(sparse.csr_matrix, (E0.T, E1.T))\n",
    "\n",
    "    def f_obj(x0, X, y):\n",
    "        \"\"\"\n",
    "        Objective function\n",
    "        \"\"\"\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        if np.any(c > 0):\n",
    "            return BIG\n",
    "\n",
    "        #loss = -(c[idx] + np.log(np.exp(-c[idx]) - 1)).sum()\n",
    "        loss = -np.log(1 - np.exp(c)).sum()\n",
    "\n",
    "        loss += b.sum() + log_logistic(b).sum() \\\n",
    "            + log_logistic(a).sum() \\\n",
    "            + .5 * alpha * w.dot(w) - np.log(z).sum()  # penalty\n",
    "        if np.isnan(loss):\n",
    "            pass\n",
    "            #import ipdb; ipdb.set_trace()\n",
    "        return loss\n",
    "\n",
    "    def f_grad(x0, X, y):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function\n",
    "        \"\"\"\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        t1 = theta_1[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        # gradient for w\n",
    "        phi_a = phi(a)\n",
    "        phi_b = phi(b)\n",
    "        grad_w = -X[k1:].T.dot(phi_b) + X.T.dot(1 - phi_a) + alpha * w\n",
    "\n",
    "        # gradient for theta\n",
    "        idx = c > 0\n",
    "        tmp = np.empty_like(c)\n",
    "        tmp[idx] = 1. / (np.exp(-c[idx]) - 1)\n",
    "        tmp[~idx] = np.exp(c[~idx]) / (1 - np.exp(c[~idx])) # should not need\n",
    "        grad_theta = (E1 - E0)[:, k1:].dot(tmp) \\\n",
    "            + E0[:, k1:].dot(phi_b) - E0.dot(1 - phi_a)\n",
    "\n",
    "        grad_theta[:-1] += 1. / np.diff(theta_0)\n",
    "        grad_theta[1:] -= 1. / np.diff(theta_0)\n",
    "        out = np.concatenate((grad_w, grad_theta))\n",
    "        return out\n",
    "\n",
    "    def f_hess(x0, s, X, y):\n",
    "        x0 = np.asarray(x0)\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        t1 = theta_1[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        D = np.diag(phi(a) * (1 - phi(a)))\n",
    "        D_= np.diag(phi(b) * (1 - phi(b)))\n",
    "        D1 = np.diag(np.exp(-c) / (np.exp(-c) - 1) ** 2)\n",
    "        Ex = (E1 - E0)[:, k1:].toarray()\n",
    "        Ex0 = E0.toarray()\n",
    "        H_A = X[k1:].T.dot(D_).dot(X[k1:]) + X.T.dot(D).dot(X)\n",
    "        H_C = - X[k1:].T.dot(D_).dot(E0[:, k1:].T.toarray()) \\\n",
    "            - X.T.dot(D).dot(E0.T.toarray())\n",
    "        H_B = Ex.dot(D1).dot(Ex.T) + Ex0[:, k1:].dot(D_).dot(Ex0[:, k1:].T) \\\n",
    "            - Ex0.dot(D).dot(Ex0.T)\n",
    "\n",
    "        p_w = H_A.shape[0]\n",
    "        tmp0 = H_A.dot(s[:p_w]) + H_C.dot(s[p_w:])\n",
    "        tmp1 = H_C.T.dot(s[:p_w]) + H_B.dot(s[p_w:])\n",
    "        return np.concatenate((tmp0, tmp1))\n",
    "\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        import pylab as pl\n",
    "        pl.matshow(H_B)\n",
    "        pl.colorbar()\n",
    "        pl.title('True')\n",
    "        import numdifftools as nd\n",
    "        Hess = nd.Hessian(lambda x: f_obj(x, X, y))\n",
    "        H = Hess(x0)\n",
    "        pl.matshow(H[H_A.shape[0]:, H_A.shape[0]:])\n",
    "        #pl.matshow()\n",
    "        pl.title('estimated')\n",
    "        pl.colorbar()\n",
    "        pl.show()\n",
    "\n",
    "\n",
    "    def grad_hess(x0, X, y):\n",
    "        grad = f_grad(x0, X, y)\n",
    "        hess = lambda x: f_hess(x0, x, X, y)\n",
    "        return grad, hess\n",
    "\n",
    "    x0 = np.random.randn(X.shape[1] + unique_y.size) / X.shape[1]\n",
    "    if w0 is not None:\n",
    "        x0[:X.shape[1]] = w0\n",
    "    else:\n",
    "        x0[:X.shape[1]] = 0.\n",
    "    x0[X.shape[1]:] = np.sort(unique_y.size * np.random.rand(unique_y.size))\n",
    "\n",
    "    #print('Check grad: %s' % optimize.check_grad(f_obj, f_grad, x0, X, y))\n",
    "    #print(optimize.approx_fprime(x0, f_obj, 1e-6, X, y))\n",
    "    #print(f_grad(x0, X, y))\n",
    "    #print(optimize.approx_fprime(x0, f_obj, 1e-6, X, y) - f_grad(x0, X, y))\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "\n",
    "    def callback(x0):\n",
    "        x0 = np.asarray(x0)\n",
    "        # print('Check grad: %s' % optimize.check_grad(f_obj, f_grad, x0, X, y))\n",
    "        if verbose:\n",
    "        # check that gradient is correctly computed\n",
    "            print('OBJ: %s' % f_obj(x0, X, y))\n",
    "\n",
    "    if solver == 'TRON':\n",
    "        import pytron\n",
    "        out = pytron.minimize(f_obj, grad_hess, x0, args=(X, y))\n",
    "    else:\n",
    "        options = {'maxiter' : max_iter, 'disp': 0, 'maxfun':10000}\n",
    "        out = optimize.minimize(f_obj, x0, args=(X, y), method=solver,\n",
    "            jac=f_grad, hessp=f_hess, options=options, callback=callback)\n",
    "\n",
    "    if not out.success:\n",
    "        warnings.warn(out.message)\n",
    "    w, theta = np.split(out.x, [X.shape[1]])\n",
    "    return w, theta\n",
    "\n",
    "\n",
    "def ordinal_logistic_predict(w, theta, X):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : coefficients obtained by ordinal_logistic\n",
    "    theta : thresholds\n",
    "    \"\"\"\n",
    "    unique_theta = np.sort(np.unique(theta))\n",
    "    out = X.dot(w)\n",
    "    unique_theta[-1] = np.inf # p(y <= max_level) = 1\n",
    "    tmp = out[:, None].repeat(unique_theta.size, axis=1)\n",
    "    return np.argmax(tmp < unique_theta, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mord import LogisticIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ir = LogisticIT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./Input/Debugging.txt\"\n",
    "data_headers = [\"Expertise\",\"Guided\",\"Outcome\"]\n",
    "dta = pd.read_csv(DATASET_PATH, names=data_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dta[data_headers[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dta[data_headers[-1]]\n",
    "y = np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticIT(alpha=1.0, max_iter=1000, verbose=0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ir.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70370370370370372"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ir.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJ: 32.6359663173\n",
      "OBJ: 28.3246468692\n",
      "OBJ: 27.8350762492\n",
      "OBJ: 27.7206192604\n",
      "OBJ: 27.5692483198\n",
      "OBJ: 26.3686138771\n",
      "OBJ: 26.2750961774\n",
      "OBJ: 26.2737698183\n",
      "OBJ: 26.2736109688\n",
      "OBJ: 26.2734834622\n",
      "OBJ: 26.2733268217\n",
      "OBJ: 26.2724653612\n",
      "OBJ: 26.2724505126\n",
      "OBJ: 26.2724500781\n",
      "OBJ: 26.2724494823\n",
      "OBJ: 26.2724491182\n",
      "OBJ: 26.2724491171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maryam/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:219: OptimizeWarning: Unknown solver options: maxfun\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, theta = ordinal_logistic_fit(X, y, verbose=True,\n",
    "                                        solver='TNC')\n",
    "\n",
    "Ir.predict(dta[['Expertise','Guided']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 0 2 0 0 0 1 1 1 0 0 1 0 1 0 2 0 0 1 0 2 0]\n",
      "[0 0 1 0 1 0 1 2 2 0 0 0 0 0 0 0 2 0 0 0 2 0 0 2 0 1 0]\n",
      "0.444444444444\n"
     ]
    }
   ],
   "source": [
    "pred = ordinal_logistic_predict(w, theta, X)\n",
    "print(pred)\n",
    "print(y)\n",
    "s = metrics.mean_absolute_error(y, pred)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23871273,  1.46570616])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ir.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Expertise</td>\n",
       "      <td>0.238713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Guided</td>\n",
       "      <td>1.465706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1\n",
       "0  Expertise  0.238713\n",
       "1     Guided  1.465706"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(X.columns, np.transpose(Ir.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
