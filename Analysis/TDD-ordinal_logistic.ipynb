{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of logistic ordinal regression (aka proportional odds) model\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy import linalg, optimize, sparse\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "BIG = 1e10\n",
    "SMALL = 1e-12\n",
    "\n",
    "\n",
    "def phi(t):\n",
    "    \"\"\"\n",
    "    logistic function, returns 1 / (1 + exp(-t))\n",
    "    \"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.empty(t.size, dtype=np.float)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "def log_logistic(t):\n",
    "    \"\"\"\n",
    "    (minus) logistic loss function, returns log(1 / (1 + exp(-t)))\n",
    "    \"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.zeros_like(t)\n",
    "    out[idx] = np.log(1 + np.exp(-t[idx]))\n",
    "    out[~idx] = (-t[~idx] + np.log(1 + np.exp(t[~idx])))\n",
    "    return out\n",
    "\n",
    "\n",
    "def ordinal_logistic_fit(X, y, alpha=0, l1_ratio=0, n_class=None, max_iter=10000,\n",
    "                         verbose=False, solver='TNC', w0=None):\n",
    "    \"\"\"\n",
    "    Ordinal logistic regression or proportional odds model.\n",
    "    Uses scipy's optimize.fmin_slsqp solver.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array, sparse matrix}, shape (n_samples, n_feaures)\n",
    "        Input data\n",
    "    y : array-like\n",
    "        Target values\n",
    "    max_iter : int\n",
    "        Maximum number of iterations\n",
    "    verbose: bool\n",
    "        Print convergence information\n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape (n_features,)\n",
    "        coefficients of the linear model\n",
    "    theta : array, shape (k,), where k is the different values of y\n",
    "        vector of thresholds\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    w0 = None\n",
    "\n",
    "    if not X.shape[0] == y.shape[0]:\n",
    "        raise ValueError('Wrong shape for X and y')\n",
    "\n",
    "    # .. order input ..\n",
    "    idx = np.argsort(y)\n",
    "    idx_inv = np.zeros_like(idx)\n",
    "    idx_inv[idx] = np.arange(idx.size)\n",
    "    X = X[idx]\n",
    "    y = y[idx].astype(np.int)\n",
    "    # make them continuous and start at zero\n",
    "    unique_y = np.unique(y)\n",
    "    for i, u in enumerate(unique_y):\n",
    "        y[y == u] = i\n",
    "    unique_y = np.unique(y)\n",
    "\n",
    "    # .. utility arrays used in f_grad ..\n",
    "    alpha = 0.\n",
    "    k1 = np.sum(y == unique_y[0])\n",
    "    E0 = (y[:, np.newaxis] == np.unique(y)).astype(np.int)\n",
    "    E1 = np.roll(E0, -1, axis=-1)\n",
    "    E1[:, -1] = 0.\n",
    "    E0, E1 = map(sparse.csr_matrix, (E0.T, E1.T))\n",
    "\n",
    "    def f_obj(x0, X, y):\n",
    "        \"\"\"\n",
    "        Objective function\n",
    "        \"\"\"\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        if np.any(c > 0):\n",
    "            return BIG\n",
    "\n",
    "        #loss = -(c[idx] + np.log(np.exp(-c[idx]) - 1)).sum()\n",
    "        loss = -np.log(1 - np.exp(c)).sum()\n",
    "\n",
    "        loss += b.sum() + log_logistic(b).sum() \\\n",
    "            + log_logistic(a).sum() \\\n",
    "            + .5 * alpha * w.dot(w) - np.log(z).sum()  # penalty\n",
    "        if np.isnan(loss):\n",
    "            pass\n",
    "            #import ipdb; ipdb.set_trace()\n",
    "        return loss\n",
    "\n",
    "    def f_grad(x0, X, y):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function\n",
    "        \"\"\"\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        t1 = theta_1[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        # gradient for w\n",
    "        phi_a = phi(a)\n",
    "        phi_b = phi(b)\n",
    "        grad_w = -X[k1:].T.dot(phi_b) + X.T.dot(1 - phi_a) + alpha * w\n",
    "\n",
    "        # gradient for theta\n",
    "        idx = c > 0\n",
    "        tmp = np.empty_like(c)\n",
    "        tmp[idx] = 1. / (np.exp(-c[idx]) - 1)\n",
    "        tmp[~idx] = np.exp(c[~idx]) / (1 - np.exp(c[~idx])) # should not need\n",
    "        grad_theta = (E1 - E0)[:, k1:].dot(tmp) \\\n",
    "            + E0[:, k1:].dot(phi_b) - E0.dot(1 - phi_a)\n",
    "\n",
    "        grad_theta[:-1] += 1. / np.diff(theta_0)\n",
    "        grad_theta[1:] -= 1. / np.diff(theta_0)\n",
    "        out = np.concatenate((grad_w, grad_theta))\n",
    "        return out\n",
    "\n",
    "    def f_hess(x0, s, X, y):\n",
    "        x0 = np.asarray(x0)\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        t1 = theta_1[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        D = np.diag(phi(a) * (1 - phi(a)))\n",
    "        D_= np.diag(phi(b) * (1 - phi(b)))\n",
    "        D1 = np.diag(np.exp(-c) / (np.exp(-c) - 1) ** 2)\n",
    "        Ex = (E1 - E0)[:, k1:].toarray()\n",
    "        Ex0 = E0.toarray()\n",
    "        H_A = X[k1:].T.dot(D_).dot(X[k1:]) + X.T.dot(D).dot(X)\n",
    "        H_C = - X[k1:].T.dot(D_).dot(E0[:, k1:].T.toarray()) \\\n",
    "            - X.T.dot(D).dot(E0.T.toarray())\n",
    "        H_B = Ex.dot(D1).dot(Ex.T) + Ex0[:, k1:].dot(D_).dot(Ex0[:, k1:].T) \\\n",
    "            - Ex0.dot(D).dot(Ex0.T)\n",
    "\n",
    "        p_w = H_A.shape[0]\n",
    "        tmp0 = H_A.dot(s[:p_w]) + H_C.dot(s[p_w:])\n",
    "        tmp1 = H_C.T.dot(s[:p_w]) + H_B.dot(s[p_w:])\n",
    "        return np.concatenate((tmp0, tmp1))\n",
    "\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        import pylab as pl\n",
    "        pl.matshow(H_B)\n",
    "        pl.colorbar()\n",
    "        pl.title('True')\n",
    "        import numdifftools as nd\n",
    "        Hess = nd.Hessian(lambda x: f_obj(x, X, y))\n",
    "        H = Hess(x0)\n",
    "        pl.matshow(H[H_A.shape[0]:, H_A.shape[0]:])\n",
    "        #pl.matshow()\n",
    "        pl.title('estimated')\n",
    "        pl.colorbar()\n",
    "        pl.show()\n",
    "\n",
    "\n",
    "    def grad_hess(x0, X, y):\n",
    "        grad = f_grad(x0, X, y)\n",
    "        hess = lambda x: f_hess(x0, x, X, y)\n",
    "        return grad, hess\n",
    "\n",
    "    x0 = np.random.randn(X.shape[1] + unique_y.size) / X.shape[1]\n",
    "    if w0 is not None:\n",
    "        x0[:X.shape[1]] = w0\n",
    "    else:\n",
    "        x0[:X.shape[1]] = 0.\n",
    "    x0[X.shape[1]:] = np.sort(unique_y.size * np.random.rand(unique_y.size))\n",
    "\n",
    "    #print('Check grad: %s' % optimize.check_grad(f_obj, f_grad, x0, X, y))\n",
    "    #print(optimize.approx_fprime(x0, f_obj, 1e-6, X, y))\n",
    "    #print(f_grad(x0, X, y))\n",
    "    #print(optimize.approx_fprime(x0, f_obj, 1e-6, X, y) - f_grad(x0, X, y))\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "\n",
    "    def callback(x0):\n",
    "        x0 = np.asarray(x0)\n",
    "        # print('Check grad: %s' % optimize.check_grad(f_obj, f_grad, x0, X, y))\n",
    "        if verbose:\n",
    "        # check that gradient is correctly computed\n",
    "            print('OBJ: %s' % f_obj(x0, X, y))\n",
    "\n",
    "    if solver == 'TRON':\n",
    "        import pytron\n",
    "        out = pytron.minimize(f_obj, grad_hess, x0, args=(X, y))\n",
    "    else:\n",
    "        options = {'maxiter' : max_iter, 'disp': 0, 'maxfun':10000}\n",
    "        out = optimize.minimize(f_obj, x0, args=(X, y), method=solver,\n",
    "            jac=f_grad, hessp=f_hess, options=options, callback=callback)\n",
    "\n",
    "    if not out.success:\n",
    "        warnings.warn(out.message)\n",
    "    w, theta = np.split(out.x, [X.shape[1]])\n",
    "    return w, theta\n",
    "\n",
    "\n",
    "def ordinal_logistic_predict(w, theta, X):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : coefficients obtained by ordinal_logistic\n",
    "    theta : thresholds\n",
    "    \"\"\"\n",
    "    unique_theta = np.sort(np.unique(theta))\n",
    "    out = X.dot(w)\n",
    "    unique_theta[-1] = np.inf # p(y <= max_level) = 1\n",
    "    tmp = out[:, None].repeat(unique_theta.size, axis=1)\n",
    "    return np.argmax(tmp < unique_theta, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "    Compare the prediction accuracy of different models on the boston dataset\n",
      "================================================================================\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    DOC = \"\"\"\n",
    "================================================================================\n",
    "    Compare the prediction accuracy of different models on the boston dataset\n",
    "================================================================================\n",
    "    \"\"\"\n",
    "    print(DOC)\n",
    "    from sklearn import cross_validation, datasets\n",
    "    boston = datasets.load_boston()\n",
    "#     print(boston.data,boston.feature_names, boston.target)\n",
    "    X, y = boston.data, np.round(boston.target)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X -= X.mean()\n",
    "y -= y.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = cross_validation.ShuffleSplit(y.size, n_iter=50, test_size=0, random_state=0)\n",
    "score_logistic = []\n",
    "score_ordinal_logistic = []\n",
    "score_ridge = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(506, n_iter=50, test_size=0, random_state=0)\n",
      "[ 14  24  25  27  31  44  45  55  57  77  88 103 111 130 141 160 168 175\n",
      " 183 203 227 255 256 271 275 279 287 289 291 296 299 308 311 314 319 326\n",
      " 330 363 368 369 373 386 387 406 423 426 462 466 469 472 478]\n"
     ]
    }
   ],
   "source": [
    "print(cv)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maryam/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/maryam/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:219: OptimizeWarning: Unknown solver options: maxfun\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJ: 3836.39643989\n",
      "OBJ: 3242.45506778\n",
      "OBJ: 2888.61515037\n",
      "OBJ: 2724.77110508\n",
      "OBJ: 2661.27089819\n",
      "OBJ: 2469.71354914\n",
      "OBJ: 2318.18112918\n",
      "OBJ: 2237.17077769\n",
      "OBJ: 2184.72474691\n",
      "OBJ: 1981.83573563\n",
      "OBJ: 1924.187787\n",
      "OBJ: 1878.99363192\n",
      "OBJ: 1817.96615032\n",
      "OBJ: 1812.14228582\n",
      "OBJ: 1805.16100241\n",
      "OBJ: 1665.3030419\n",
      "OBJ: 1639.31976828\n",
      "OBJ: 1626.38841872\n",
      "OBJ: 1613.44829009\n",
      "OBJ: 1592.85324032\n",
      "OBJ: 1575.59259395\n",
      "OBJ: 1560.55013888\n",
      "OBJ: 1525.98550451\n",
      "OBJ: 1509.81084072\n",
      "OBJ: 1490.75931305\n",
      "OBJ: 1464.54204676\n",
      "OBJ: 1421.91921257\n",
      "OBJ: 1394.78716069\n",
      "OBJ: 1387.28477631\n",
      "OBJ: 1372.06857887\n",
      "OBJ: 1365.77496971\n",
      "OBJ: 1359.89368138\n",
      "OBJ: 1359.31691479\n",
      "OBJ: 1359.02151926\n",
      "OBJ: 1356.01272689\n",
      "OBJ: 1355.5537549\n",
      "OBJ: 1355.49568123\n",
      "OBJ: 1352.24027894\n",
      "OBJ: 1349.4168109\n",
      "OBJ: 1348.8463026\n",
      "OBJ: 1346.68566515\n",
      "OBJ: 1346.57088012\n",
      "OBJ: 1345.85195546\n",
      "OBJ: 1345.70859976\n",
      "OBJ: 1344.97692805\n",
      "OBJ: 1344.88934875\n",
      "OBJ: 1343.27988949\n",
      "OBJ: 1341.50544428\n",
      "OBJ: 1335.57424913\n",
      "OBJ: 1334.68653214\n",
      "OBJ: 1333.20707535\n",
      "OBJ: 1332.92941811\n",
      "OBJ: 1332.45371135\n",
      "OBJ: 1332.20834752\n",
      "OBJ: 1332.16694567\n",
      "OBJ: 1332.13342377\n",
      "OBJ: 1332.11816988\n",
      "OBJ: 1332.1122266\n",
      "OBJ: 1332.09707038\n",
      "OBJ: 1332.06234682\n",
      "OBJ: 1331.99486184\n",
      "OBJ: 1331.71649008\n",
      "OBJ: 1331.65333232\n",
      "OBJ: 1331.47643932\n",
      "OBJ: 1330.58103856\n",
      "OBJ: 1329.56941478\n",
      "OBJ: 1329.5233669\n",
      "OBJ: 1328.35063558\n",
      "OBJ: 1327.64420256\n",
      "OBJ: 1327.49969892\n",
      "OBJ: 1327.48059738\n",
      "OBJ: 1327.35830007\n",
      "OBJ: 1327.23476756\n",
      "OBJ: 1327.2133573\n",
      "OBJ: 1326.91896866\n",
      "OBJ: 1326.6465792\n",
      "OBJ: 1326.61703237\n",
      "OBJ: 1326.58326881\n",
      "OBJ: 1326.5823128\n",
      "OBJ: 1326.5813437\n",
      "OBJ: 1326.58088042\n",
      "OBJ: 1326.57978473\n",
      "OBJ: 1326.57047593\n",
      "OBJ: 1326.56882569\n",
      "OBJ: 1326.56093505\n",
      "OBJ: 1326.55676104\n",
      "OBJ: 1326.55567745\n",
      "OBJ: 1326.55026869\n",
      "OBJ: 1326.53931586\n",
      "OBJ: 1326.53692593\n",
      "OBJ: 1326.52880769\n",
      "OBJ: 1326.52615575\n",
      "OBJ: 1326.52007851\n",
      "OBJ: 1326.5186534\n",
      "OBJ: 1326.51803397\n",
      "OBJ: 1326.50879013\n",
      "OBJ: 1326.50447958\n",
      "OBJ: 1326.50334544\n",
      "OBJ: 1326.49825071\n",
      "OBJ: 1326.4910593\n",
      "OBJ: 1326.48205523\n",
      "OBJ: 1326.4798275\n",
      "OBJ: 1326.47603904\n",
      "OBJ: 1326.47272102\n",
      "OBJ: 1326.47177471\n",
      "OBJ: 1326.47043591\n",
      "OBJ: 1326.46978872\n",
      "OBJ: 1326.46960221\n",
      "OBJ: 1326.46949392\n",
      "OBJ: 1326.46908453\n",
      "OBJ: 1326.46770718\n",
      "OBJ: 1326.46704195\n",
      "OBJ: 1326.46692036\n",
      "OBJ: 1326.46517596\n",
      "OBJ: 1326.46450997\n",
      "OBJ: 1326.46407696\n",
      "OBJ: 1326.46363135\n",
      "OBJ: 1326.46349985\n",
      "OBJ: 1326.46344226\n",
      "OBJ: 1326.46340588\n",
      "OBJ: 1326.46326553\n",
      "OBJ: 1326.46294884\n",
      "OBJ: 1326.4628853\n",
      "OBJ: 1326.46274951\n",
      "OBJ: 1326.46126388\n",
      "OBJ: 1326.45364497\n",
      "OBJ: 1326.4456968\n",
      "OBJ: 1326.4440487\n",
      "OBJ: 1326.42545357\n",
      "OBJ: 1326.41582918\n",
      "OBJ: 1326.40047298\n",
      "OBJ: 1326.39764305\n",
      "OBJ: 1326.39411585\n",
      "OBJ: 1326.39330212\n",
      "OBJ: 1326.3902821\n",
      "OBJ: 1326.39000363\n",
      "OBJ: 1326.38955478\n",
      "OBJ: 1326.38665693\n",
      "OBJ: 1326.38130358\n",
      "OBJ: 1326.38023481\n",
      "OBJ: 1326.37394032\n",
      "OBJ: 1326.37333292\n",
      "OBJ: 1326.37071858\n",
      "OBJ: 1326.37045004\n",
      "OBJ: 1326.37004301\n",
      "OBJ: 1326.36945807\n",
      "OBJ: 1326.36939824\n",
      "OBJ: 1326.36755411\n",
      "OBJ: 1326.36708258\n",
      "OBJ: 1326.36354844\n",
      "OBJ: 1326.3562856\n",
      "OBJ: 1326.34847642\n",
      "OBJ: 1326.34690163\n",
      "OBJ: 1326.34631883\n",
      "OBJ: 1326.34403228\n",
      "OBJ: 1326.3413414\n",
      "OBJ: 1326.34085695\n",
      "OBJ: 1326.33581258\n",
      "OBJ: 1326.33517416\n",
      "OBJ: 1326.33378251\n",
      "OBJ: 1326.3336795\n",
      "OBJ: 1326.33327077\n",
      "OBJ: 1326.33301015\n",
      "OBJ: 1326.33285697\n",
      "OBJ: 1326.33250668\n",
      "OBJ: 1326.3323706\n",
      "OBJ: 1326.3322071\n",
      "OBJ: 1326.33105747\n",
      "OBJ: 1326.32703955\n",
      "OBJ: 1326.32326552\n",
      "OBJ: 1326.31923233\n",
      "OBJ: 1326.3126855\n",
      "OBJ: 1326.30868198\n",
      "OBJ: 1326.28867402\n",
      "OBJ: 1326.24849092\n",
      "OBJ: 1326.22944846\n",
      "OBJ: 1326.20027169\n",
      "OBJ: 1326.19833769\n",
      "OBJ: 1326.18942163\n",
      "OBJ: 1326.18672636\n",
      "OBJ: 1326.16838955\n",
      "OBJ: 1326.16726227\n",
      "OBJ: 1326.16091994\n",
      "OBJ: 1326.15328059\n",
      "OBJ: 1326.1504875\n",
      "OBJ: 1326.14911364\n",
      "OBJ: 1326.12535204\n",
      "OBJ: 1326.09984085\n",
      "OBJ: 1326.09585132\n",
      "OBJ: 1326.08450136\n",
      "OBJ: 1326.06898893\n",
      "OBJ: 1326.06500354\n",
      "OBJ: 1326.05640461\n",
      "OBJ: 1326.03284154\n",
      "OBJ: 1326.01165327\n",
      "OBJ: 1326.00657692\n",
      "OBJ: 1325.97944809\n",
      "OBJ: 1325.97388733\n",
      "OBJ: 1325.95280875\n",
      "OBJ: 1325.8665142\n",
      "OBJ: 1325.86121388\n",
      "OBJ: 1325.85285644\n",
      "OBJ: 1325.8391288\n",
      "OBJ: 1325.79714092\n",
      "OBJ: 1325.71767779\n",
      "OBJ: 1325.69145791\n",
      "OBJ: 1325.67994777\n",
      "OBJ: 1325.64917371\n",
      "OBJ: 1325.59042853\n",
      "OBJ: 1325.57421837\n",
      "OBJ: 1325.57090362\n",
      "OBJ: 1325.55631721\n",
      "OBJ: 1325.54675575\n",
      "OBJ: 1325.53858705\n",
      "OBJ: 1325.536623\n",
      "OBJ: 1325.52329496\n",
      "OBJ: 1325.45031002\n",
      "OBJ: 1325.42790329\n",
      "OBJ: 1325.37555895\n",
      "OBJ: 1325.36457426\n",
      "OBJ: 1325.34457984\n",
      "OBJ: 1325.34348184\n",
      "OBJ: 1325.33692695\n",
      "OBJ: 1325.32352657\n",
      "OBJ: 1325.31571189\n",
      "OBJ: 1325.3132841\n",
      "OBJ: 1325.3079092\n",
      "OBJ: 1325.30704018\n",
      "OBJ: 1325.30592554\n",
      "OBJ: 1325.30503528\n",
      "OBJ: 1325.30467928\n",
      "OBJ: 1325.30239215\n",
      "OBJ: 1325.30173181\n",
      "OBJ: 1325.29979296\n",
      "OBJ: 1325.29953362\n",
      "OBJ: 1325.29801426\n",
      "OBJ: 1325.29649074\n",
      "OBJ: 1325.29629588\n",
      "OBJ: 1325.29311836\n",
      "OBJ: 1325.28590379\n",
      "OBJ: 1325.27066442\n",
      "OBJ: 1325.25082493\n",
      "OBJ: 1325.24963673\n",
      "OBJ: 1325.24781302\n",
      "OBJ: 1325.24673705\n",
      "OBJ: 1325.24177573\n",
      "OBJ: 1325.23899294\n",
      "OBJ: 1325.23237751\n",
      "OBJ: 1325.22679335\n",
      "OBJ: 1325.225433\n",
      "OBJ: 1325.2246117\n",
      "OBJ: 1325.22421494\n",
      "OBJ: 1325.22317285\n",
      "OBJ: 1325.22289087\n",
      "OBJ: 1325.22200182\n",
      "OBJ: 1325.22178354\n",
      "OBJ: 1325.22098243\n",
      "OBJ: 1325.22070247\n",
      "OBJ: 1325.21976449\n",
      "OBJ: 1325.21706144\n",
      "OBJ: 1325.21535598\n",
      "OBJ: 1325.20919677\n",
      "OBJ: 1325.2061554\n",
      "OBJ: 1325.19289981\n",
      "OBJ: 1325.18779733\n",
      "OBJ: 1325.18652953\n",
      "OBJ: 1325.18574941\n",
      "OBJ: 1325.18560701\n",
      "OBJ: 1325.18552576\n",
      "OBJ: 1325.18396814\n",
      "OBJ: 1325.1838241\n",
      "OBJ: 1325.18324977\n",
      "OBJ: 1325.18318614\n",
      "OBJ: 1325.18265346\n",
      "OBJ: 1325.18255449\n",
      "OBJ: 1325.1822388\n",
      "OBJ: 1325.18216019\n",
      "OBJ: 1325.18176287\n",
      "OBJ: 1325.18045668\n",
      "OBJ: 1325.18004688\n",
      "OBJ: 1325.17923862\n",
      "OBJ: 1325.1776706\n",
      "OBJ: 1325.17558789\n",
      "OBJ: 1325.17498851\n",
      "OBJ: 1325.17322748\n",
      "OBJ: 1325.16611938\n",
      "OBJ: 1325.16417074\n",
      "OBJ: 1325.16307161\n",
      "OBJ: 1325.15461083\n",
      "OBJ: 1325.11482572\n",
      "OBJ: 1325.08470617\n",
      "OBJ: 1324.91346714\n",
      "OBJ: 1324.85751615\n",
      "OBJ: 1324.83730318\n",
      "OBJ: 1324.77923194\n",
      "OBJ: 1324.77664197\n",
      "OBJ: 1324.76192954\n",
      "OBJ: 1324.76080632\n",
      "OBJ: 1324.75526902\n",
      "OBJ: 1324.74077505\n",
      "OBJ: 1324.73459619\n",
      "OBJ: 1324.72805348\n",
      "OBJ: 1324.72208459\n",
      "OBJ: 1324.71602543\n",
      "OBJ: 1324.69185924\n",
      "OBJ: 1324.59733554\n",
      "OBJ: 1324.57126606\n",
      "OBJ: 1324.56619026\n",
      "OBJ: 1324.545718\n",
      "OBJ: 1324.52850248\n",
      "OBJ: 1324.51085701\n",
      "OBJ: 1324.50579644\n",
      "OBJ: 1324.50405134\n",
      "OBJ: 1324.50029326\n",
      "OBJ: 1324.49966711\n",
      "OBJ: 1324.49442351\n",
      "OBJ: 1324.49173584\n",
      "OBJ: 1324.4824676\n",
      "OBJ: 1324.47915359\n",
      "OBJ: 1324.47023129\n",
      "OBJ: 1324.45751966\n",
      "OBJ: 1324.44238335\n",
      "OBJ: 1324.44092707\n",
      "OBJ: 1324.43729744\n",
      "OBJ: 1324.43652725\n",
      "OBJ: 1324.43323523\n",
      "OBJ: 1324.42798013\n",
      "OBJ: 1324.42302055\n",
      "OBJ: 1324.39568188\n",
      "OBJ: 1324.39348595\n",
      "OBJ: 1324.3601262\n",
      "OBJ: 1324.35252602\n",
      "OBJ: 1324.27116304\n",
      "OBJ: 1324.24462501\n",
      "OBJ: 1324.20223096\n",
      "OBJ: 1324.19975351\n",
      "OBJ: 1324.10774766\n",
      "OBJ: 1324.09153393\n",
      "OBJ: 1324.05033937\n",
      "OBJ: 1324.04657605\n",
      "OBJ: 1324.03934341\n",
      "OBJ: 1324.03770443\n",
      "OBJ: 1324.03441632\n",
      "OBJ: 1324.03318073\n",
      "OBJ: 1324.02947487\n",
      "OBJ: 1324.02376718\n",
      "OBJ: 1324.01996186\n",
      "OBJ: 1324.0008\n",
      "OBJ: 1323.995975\n",
      "OBJ: 1323.9939138\n",
      "OBJ: 1323.98197337\n",
      "OBJ: 1323.98103752\n",
      "OBJ: 1323.97732676\n",
      "OBJ: 1323.97513654\n",
      "OBJ: 1323.97052586\n",
      "OBJ: 1323.96882586\n",
      "OBJ: 1323.96440999\n",
      "OBJ: 1323.96303799\n",
      "OBJ: 1323.95604141\n",
      "OBJ: 1323.95336915\n",
      "OBJ: 1323.93313348\n",
      "OBJ: 1323.92704493\n",
      "OBJ: 1323.89310162\n",
      "OBJ: 1323.88601273\n",
      "OBJ: 1323.8541519\n",
      "OBJ: 1323.84900782\n",
      "OBJ: 1323.84137822\n",
      "OBJ: 1323.84040519\n",
      "OBJ: 1323.83954546\n",
      "OBJ: 1323.83935761\n",
      "OBJ: 1323.83877151\n",
      "OBJ: 1323.83865541\n",
      "OBJ: 1323.8382113\n",
      "OBJ: 1323.8377265\n",
      "OBJ: 1323.83760873\n",
      "OBJ: 1323.83580284\n",
      "OBJ: 1323.83454276\n",
      "OBJ: 1323.83337287\n",
      "OBJ: 1323.83319233\n",
      "OBJ: 1323.83212496\n",
      "OBJ: 1323.83155345\n",
      "OBJ: 1323.8291558\n",
      "OBJ: 1323.82884125\n",
      "OBJ: 1323.82811758\n",
      "OBJ: 1323.82803615\n",
      "OBJ: 1323.82763508\n",
      "OBJ: 1323.82758474\n",
      "OBJ: 1323.82757376\n",
      "OBJ: 1323.82749011\n",
      "OBJ: 1323.82748313\n",
      "OBJ: 1323.82744209\n",
      "OBJ: 1323.82742586\n",
      "OBJ: 1323.82739\n",
      "OBJ: 1323.82738453\n",
      "OBJ: 1323.8273407\n",
      "OBJ: 1323.82732707\n",
      "OBJ: 1323.82723995\n",
      "OBJ: 1323.82703964\n",
      "OBJ: 1323.82699996\n",
      "OBJ: 1323.82684929\n",
      "OBJ: 1323.82672107\n",
      "OBJ: 1323.82670823\n",
      "OBJ: 1323.8265495\n",
      "OBJ: 1323.82631696\n",
      "OBJ: 1323.82623674\n",
      "OBJ: 1323.82579116\n",
      "OBJ: 1323.82463887\n",
      "OBJ: 1323.82452344\n",
      "OBJ: 1323.82447713\n",
      "OBJ: 1323.82433887\n",
      "OBJ: 1323.82428719\n",
      "OBJ: 1323.82425798\n",
      "OBJ: 1323.82424687\n",
      "OBJ: 1323.8242221\n",
      "OBJ: 1323.82414775\n",
      "OBJ: 1323.82413551\n",
      "OBJ: 1323.82412934\n",
      "OBJ: 1323.82407333\n",
      "OBJ: 1323.82402466\n",
      "OBJ: 1323.82401764\n",
      "OBJ: 1323.82390798\n",
      "OBJ: 1323.82371468\n",
      "OBJ: 1323.82357021\n",
      "OBJ: 1323.82343849\n",
      "OBJ: 1323.82325312\n",
      "OBJ: 1323.82318978\n",
      "OBJ: 1323.82305667\n",
      "OBJ: 1323.82260006\n",
      "OBJ: 1323.82237093\n",
      "OBJ: 1323.82181642\n",
      "OBJ: 1323.82144166\n",
      "OBJ: 1323.821333\n",
      "OBJ: 1323.82128882\n",
      "OBJ: 1323.82126153\n",
      "OBJ: 1323.82120192\n",
      "OBJ: 1323.82105859\n",
      "OBJ: 1323.82100811\n",
      "OBJ: 1323.82099844\n",
      "OBJ: 1323.82096887\n",
      "OBJ: 1323.82088577\n",
      "OBJ: 1323.82085856\n",
      "OBJ: 1323.82085029\n",
      "OBJ: 1323.82084661\n",
      "OBJ: 1323.82076612\n",
      "OBJ: 1323.82063507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJ: 1323.8205302\n",
      "OBJ: 1323.82036889\n",
      "OBJ: 1323.81996586\n",
      "OBJ: 1323.81984229\n",
      "OBJ: 1323.81982114\n",
      "OBJ: 1323.81919644\n",
      "OBJ: 1323.81809386\n",
      "OBJ: 1323.81621857\n",
      "OBJ: 1323.81566799\n",
      "OBJ: 1323.81548423\n",
      "OBJ: 1323.81463822\n",
      "OBJ: 1323.81417395\n",
      "OBJ: 1323.8064239\n",
      "OBJ: 1323.79852642\n",
      "OBJ: 1323.79726517\n",
      "OBJ: 1323.78892081\n",
      "OBJ: 1323.78502837\n",
      "OBJ: 1323.7846583\n",
      "OBJ: 1323.78456481\n",
      "OBJ: 1323.78230207\n",
      "OBJ: 1323.78216071\n",
      "OBJ: 1323.78191975\n",
      "OBJ: 1323.78190143\n",
      "OBJ: 1323.7816807\n",
      "OBJ: 1323.78150956\n",
      "OBJ: 1323.78122066\n",
      "OBJ: 1323.78065098\n",
      "OBJ: 1323.77978567\n",
      "OBJ: 1323.77887045\n",
      "OBJ: 1323.7775291\n",
      "OBJ: 1323.77585735\n",
      "OBJ: 1323.77538373\n",
      "OBJ: 1323.77374621\n",
      "OBJ: 1323.77341919\n",
      "OBJ: 1323.77102369\n",
      "OBJ: 1323.76471952\n",
      "OBJ: 1323.76422559\n",
      "OBJ: 1323.7641506\n",
      "OBJ: 1323.76322127\n",
      "OBJ: 1323.76310571\n",
      "OBJ: 1323.76273785\n",
      "OBJ: 1323.76254786\n",
      "OBJ: 1323.76253237\n",
      "OBJ: 1323.7622901\n",
      "OBJ: 1323.76139327\n",
      "OBJ: 1323.76127596\n",
      "OBJ: 1323.76121512\n",
      "OBJ: 1323.76090312\n",
      "OBJ: 1323.76030869\n",
      "OBJ: 1323.76023974\n",
      "OBJ: 1323.75956975\n",
      "OBJ: 1323.75794406\n",
      "OBJ: 1323.75718829\n",
      "OBJ: 1323.75503753\n",
      "OBJ: 1323.75480336\n",
      "OBJ: 1323.75282587\n",
      "OBJ: 1323.75173022\n",
      "OBJ: 1323.75146288\n",
      "OBJ: 1323.7468439\n",
      "OBJ: 1323.74369707\n",
      "OBJ: 1323.74166713\n",
      "OBJ: 1323.74123219\n",
      "OBJ: 1323.74020266\n",
      "OBJ: 1323.73982115\n",
      "OBJ: 1323.73974757\n",
      "OBJ: 1323.73951026\n",
      "OBJ: 1323.73947271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maryam/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:222: UserWarning: Max. number of function evaluations reached\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c31f4a7d4163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                                     solver='TNC')\n\u001b[1;32m     11\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mordinal_logistic_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR (ORDINAL)  fold %s: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "for i, (train, test) in enumerate(cv):\n",
    "    #test = train\n",
    "    if not np.all(np.unique(y[train]) == np.unique(y)):\n",
    "        # we need the train set to have all different classes\n",
    "        continue\n",
    "    assert np.all(np.unique(y[train]) == np.unique(y))\n",
    "    train = np.sort(train)\n",
    "    test = np.sort(test)\n",
    "    w, theta = ordinal_logistic_fit(X[train], y[train], verbose=True,\n",
    "                                    solver='TNC')\n",
    "    pred = ordinal_logistic_predict(w, theta, X[test])\n",
    "    1/0\n",
    "    s = metrics.mean_absolute_error(y[test], pred)\n",
    "    print('ERROR (ORDINAL)  fold %s: %s' % (i+1, s))\n",
    "    score_ordinal_logistic.append(s)\n",
    "\n",
    "    from sklearn import linear_model\n",
    "    clf = linear_model.LogisticRegression(C=1.)\n",
    "    clf.fit(X[train], y[train])\n",
    "    pred = clf.predict(X[test])\n",
    "    s = metrics.mean_absolute_error(y[test], pred)\n",
    "    print('ERROR (LOGISTIC) fold %s: %s' % (i+1, s))\n",
    "    score_logistic.append(s)\n",
    "\n",
    "    from sklearn import linear_model\n",
    "    clf = linear_model.Ridge(alpha=1.)\n",
    "    clf.fit(X[train], y[train])\n",
    "    pred = np.round(clf.predict(X[test]))\n",
    "    s = metrics.mean_absolute_error(y[test], pred)\n",
    "    print('ERROR (RIDGE)    fold %s: %s' % (i+1, s))\n",
    "    score_ridge.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maryam/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  del sys.path[0]\n",
      "/Users/maryam/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:219: OptimizeWarning: Unknown solver options: maxfun\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJ: 4393.47818448\n",
      "OBJ: 3169.54741094\n",
      "OBJ: 2865.91348681\n",
      "OBJ: 2675.46758506\n",
      "OBJ: 2419.49848579\n",
      "OBJ: 2292.07167923\n",
      "OBJ: 2090.46101133\n",
      "OBJ: 2024.75507852\n",
      "OBJ: 1971.3519521\n",
      "OBJ: 1934.81872919\n",
      "OBJ: 1916.04367528\n",
      "OBJ: 1878.0824913\n",
      "OBJ: 1850.95467247\n",
      "OBJ: 1835.24928112\n",
      "OBJ: 1793.52710795\n",
      "OBJ: 1771.83390253\n",
      "OBJ: 1698.34628659\n",
      "OBJ: 1674.87511243\n",
      "OBJ: 1662.88922312\n",
      "OBJ: 1610.01719669\n",
      "OBJ: 1588.10360517\n",
      "OBJ: 1546.09939429\n",
      "OBJ: 1534.38256332\n",
      "OBJ: 1511.64420543\n",
      "OBJ: 1480.60654913\n",
      "OBJ: 1421.03520876\n",
      "OBJ: 1407.25998994\n",
      "OBJ: 1373.65097995\n",
      "OBJ: 1369.61919213\n",
      "OBJ: 1366.19775492\n",
      "OBJ: 1356.74050864\n",
      "OBJ: 1353.26677039\n",
      "OBJ: 1352.8873939\n",
      "OBJ: 1352.3779472\n",
      "OBJ: 1352.03780058\n",
      "OBJ: 1348.92257291\n",
      "OBJ: 1345.07326649\n",
      "OBJ: 1343.05991139\n",
      "OBJ: 1342.62394393\n",
      "OBJ: 1341.50507081\n",
      "OBJ: 1341.46759461\n",
      "OBJ: 1341.45509326\n",
      "OBJ: 1341.22629242\n",
      "OBJ: 1341.04062816\n",
      "OBJ: 1340.54086932\n",
      "OBJ: 1340.1022632\n",
      "OBJ: 1339.16323748\n",
      "OBJ: 1338.92314042\n",
      "OBJ: 1337.49895166\n",
      "OBJ: 1335.32412578\n",
      "OBJ: 1334.9954297\n",
      "OBJ: 1334.6988906\n",
      "OBJ: 1332.97892914\n",
      "OBJ: 1332.48921083\n",
      "OBJ: 1332.25970369\n",
      "OBJ: 1332.12413998\n",
      "OBJ: 1330.77484176\n",
      "OBJ: 1330.39034039\n",
      "OBJ: 1330.31350725\n",
      "OBJ: 1330.27774783\n",
      "OBJ: 1329.74088001\n",
      "OBJ: 1329.67046986\n",
      "OBJ: 1329.64139393\n",
      "OBJ: 1329.62516037\n",
      "OBJ: 1329.54648018\n",
      "OBJ: 1329.50894739\n",
      "OBJ: 1329.45110579\n",
      "OBJ: 1329.43897209\n",
      "OBJ: 1329.26694649\n",
      "OBJ: 1329.07171582\n",
      "OBJ: 1328.59748194\n",
      "OBJ: 1328.43186899\n",
      "OBJ: 1328.18154256\n",
      "OBJ: 1327.88088078\n",
      "OBJ: 1327.56880334\n",
      "OBJ: 1327.42168008\n",
      "OBJ: 1327.37485249\n",
      "OBJ: 1327.21863472\n",
      "OBJ: 1327.18991008\n",
      "OBJ: 1327.14723581\n",
      "OBJ: 1327.10605758\n",
      "OBJ: 1327.0978518\n",
      "OBJ: 1326.98771736\n",
      "OBJ: 1326.59723761\n",
      "OBJ: 1326.42051984\n",
      "OBJ: 1326.39101143\n",
      "OBJ: 1326.30543658\n",
      "OBJ: 1326.24499493\n",
      "OBJ: 1326.16786626\n",
      "OBJ: 1326.14371688\n",
      "OBJ: 1326.06105284\n",
      "OBJ: 1326.05352723\n",
      "OBJ: 1326.04433099\n",
      "OBJ: 1326.01639358\n",
      "OBJ: 1325.95855181\n",
      "OBJ: 1325.93089182\n",
      "OBJ: 1325.92191111\n",
      "OBJ: 1325.89256899\n",
      "OBJ: 1325.85759724\n",
      "OBJ: 1325.77876357\n",
      "OBJ: 1325.70985058\n",
      "OBJ: 1325.69324641\n",
      "OBJ: 1325.57626329\n",
      "OBJ: 1325.55183956\n",
      "OBJ: 1325.52558585\n",
      "OBJ: 1325.37767505\n",
      "OBJ: 1325.3291926\n",
      "OBJ: 1325.22944885\n",
      "OBJ: 1325.20683529\n",
      "OBJ: 1325.15280031\n",
      "OBJ: 1325.13103389\n",
      "OBJ: 1325.09582029\n",
      "OBJ: 1325.08393823\n",
      "OBJ: 1325.07983125\n",
      "OBJ: 1325.04767805\n",
      "OBJ: 1325.03930473\n",
      "OBJ: 1325.01710694\n",
      "OBJ: 1325.00606747\n",
      "OBJ: 1324.97678058\n",
      "OBJ: 1324.9500378\n",
      "OBJ: 1324.92604188\n",
      "OBJ: 1324.92265485\n",
      "OBJ: 1324.9210752\n",
      "OBJ: 1324.9174024\n",
      "OBJ: 1324.91700092\n",
      "OBJ: 1324.91658571\n",
      "OBJ: 1324.91247688\n",
      "OBJ: 1324.90210663\n",
      "OBJ: 1324.90128921\n",
      "OBJ: 1324.90031171\n",
      "OBJ: 1324.88473922\n",
      "OBJ: 1324.88204176\n",
      "OBJ: 1324.87718451\n",
      "OBJ: 1324.87486114\n",
      "OBJ: 1324.86953305\n",
      "OBJ: 1324.86862562\n",
      "OBJ: 1324.86381487\n",
      "OBJ: 1324.86340502\n",
      "OBJ: 1324.8567035\n",
      "OBJ: 1324.85042565\n",
      "OBJ: 1324.83119446\n",
      "OBJ: 1324.82408227\n",
      "OBJ: 1324.81421623\n",
      "OBJ: 1324.80888821\n",
      "OBJ: 1324.80333817\n",
      "OBJ: 1324.80281609\n",
      "OBJ: 1324.80011743\n",
      "OBJ: 1324.79979072\n",
      "OBJ: 1324.79946066\n",
      "OBJ: 1324.799177\n",
      "OBJ: 1324.79902335\n",
      "OBJ: 1324.79873696\n",
      "OBJ: 1324.79844845\n",
      "OBJ: 1324.79816463\n",
      "OBJ: 1324.79803024\n",
      "OBJ: 1324.79610405\n",
      "OBJ: 1324.79397691\n",
      "OBJ: 1324.78783542\n",
      "OBJ: 1324.75983676\n",
      "OBJ: 1324.74187228\n",
      "OBJ: 1324.70570402\n",
      "OBJ: 1324.69441234\n",
      "OBJ: 1324.67084041\n",
      "OBJ: 1324.63993523\n",
      "OBJ: 1324.63473367\n",
      "OBJ: 1324.63087394\n",
      "OBJ: 1324.60554678\n",
      "OBJ: 1324.5992043\n",
      "OBJ: 1324.53706389\n",
      "OBJ: 1324.53221145\n",
      "OBJ: 1324.52057901\n",
      "OBJ: 1324.51601869\n",
      "OBJ: 1324.49946418\n",
      "OBJ: 1324.48958603\n",
      "OBJ: 1324.48745891\n",
      "OBJ: 1324.48682371\n",
      "OBJ: 1324.48611925\n",
      "OBJ: 1324.48540827\n",
      "OBJ: 1324.4851093\n",
      "OBJ: 1324.48503268\n",
      "OBJ: 1324.48499899\n",
      "OBJ: 1324.48457178\n",
      "OBJ: 1324.48423436\n",
      "OBJ: 1324.48299481\n",
      "OBJ: 1324.48261849\n",
      "OBJ: 1324.47508877\n",
      "OBJ: 1324.46790187\n",
      "OBJ: 1324.46572657\n",
      "OBJ: 1324.46288045\n",
      "OBJ: 1324.46101736\n",
      "OBJ: 1324.46040585\n",
      "OBJ: 1324.45916987\n",
      "OBJ: 1324.45678603\n",
      "OBJ: 1324.45609585\n",
      "OBJ: 1324.45576956\n",
      "OBJ: 1324.45441061\n",
      "OBJ: 1324.45231108\n",
      "OBJ: 1324.45100122\n",
      "OBJ: 1324.45058457\n",
      "OBJ: 1324.44892501\n",
      "OBJ: 1324.44832091\n",
      "OBJ: 1324.44806299\n",
      "OBJ: 1324.44627225\n",
      "OBJ: 1324.44302969\n",
      "OBJ: 1324.4058181\n",
      "OBJ: 1324.30492887\n",
      "OBJ: 1324.27928394\n",
      "OBJ: 1324.26344006\n",
      "OBJ: 1324.24906535\n",
      "OBJ: 1324.21854191\n",
      "OBJ: 1324.20751537\n",
      "OBJ: 1324.2058824\n",
      "OBJ: 1324.20552958\n",
      "OBJ: 1324.20501168\n",
      "OBJ: 1324.20331021\n",
      "OBJ: 1324.20133317\n",
      "OBJ: 1324.19145469\n",
      "OBJ: 1324.19071134\n",
      "OBJ: 1324.18803913\n",
      "OBJ: 1324.18753385\n",
      "OBJ: 1324.18732481\n",
      "OBJ: 1324.18589796\n",
      "OBJ: 1324.18559626\n",
      "OBJ: 1324.18445359\n",
      "OBJ: 1324.18430011\n",
      "OBJ: 1324.18385974\n",
      "OBJ: 1324.18340409\n",
      "OBJ: 1324.1824243\n",
      "OBJ: 1324.17906809\n",
      "OBJ: 1324.17722414\n",
      "OBJ: 1324.16951985\n",
      "OBJ: 1324.16828274\n",
      "OBJ: 1324.15626373\n",
      "OBJ: 1324.1391795\n",
      "OBJ: 1324.12625438\n",
      "OBJ: 1324.12448191\n",
      "OBJ: 1324.12170935\n",
      "OBJ: 1324.11987669\n",
      "OBJ: 1324.11720532\n",
      "OBJ: 1324.11682293\n",
      "OBJ: 1324.11658975\n",
      "OBJ: 1324.11472445\n",
      "OBJ: 1324.11413353\n",
      "OBJ: 1324.11102694\n",
      "OBJ: 1324.10300647\n",
      "OBJ: 1324.10198147\n",
      "OBJ: 1324.09092167\n",
      "OBJ: 1324.09030935\n",
      "OBJ: 1324.07854093\n",
      "OBJ: 1324.0639548\n",
      "OBJ: 1324.04929334\n",
      "OBJ: 1324.04612087\n",
      "OBJ: 1324.04316501\n",
      "OBJ: 1324.04108278\n",
      "OBJ: 1324.01744255\n",
      "OBJ: 1324.00284698\n",
      "OBJ: 1323.97526346\n",
      "OBJ: 1323.93829227\n",
      "OBJ: 1323.92662297\n",
      "OBJ: 1323.90055844\n",
      "OBJ: 1323.89858633\n",
      "OBJ: 1323.89714481\n",
      "OBJ: 1323.88744626\n",
      "OBJ: 1323.8797241\n",
      "OBJ: 1323.87806513\n",
      "OBJ: 1323.85794513\n",
      "OBJ: 1323.85428236\n",
      "OBJ: 1323.8528135\n",
      "OBJ: 1323.84262634\n",
      "OBJ: 1323.81695366\n",
      "OBJ: 1323.79723114\n",
      "OBJ: 1323.79376923\n",
      "OBJ: 1323.67570264\n",
      "OBJ: 1323.65619146\n",
      "OBJ: 1323.56130021\n",
      "OBJ: 1323.55399724\n",
      "OBJ: 1323.53110175\n",
      "OBJ: 1323.52713242\n",
      "OBJ: 1323.52525232\n",
      "OBJ: 1323.50839216\n",
      "OBJ: 1323.50236307\n",
      "OBJ: 1323.49698192\n",
      "OBJ: 1323.49442837\n",
      "OBJ: 1323.49352744\n",
      "OBJ: 1323.485144\n",
      "OBJ: 1323.48114043\n",
      "OBJ: 1323.43359832\n",
      "OBJ: 1323.38553705\n",
      "OBJ: 1323.2399397\n",
      "OBJ: 1323.21882726\n",
      "OBJ: 1323.07472085\n",
      "OBJ: 1322.87380895\n",
      "OBJ: 1322.85061617\n",
      "OBJ: 1322.83993102\n",
      "OBJ: 1322.8203623\n",
      "OBJ: 1322.71115216\n",
      "OBJ: 1322.63782372\n",
      "OBJ: 1322.51365047\n",
      "OBJ: 1322.50313095\n",
      "OBJ: 1322.46970249\n",
      "OBJ: 1322.42138615\n",
      "OBJ: 1322.41630132\n",
      "OBJ: 1322.40756395\n",
      "OBJ: 1322.40593903\n",
      "OBJ: 1322.40452466\n",
      "OBJ: 1322.40370609\n",
      "OBJ: 1322.40164726\n",
      "OBJ: 1322.40044175\n",
      "OBJ: 1322.39947101\n",
      "OBJ: 1322.39936963\n",
      "OBJ: 1322.39934442\n",
      "OBJ: 1322.39895687\n",
      "OBJ: 1322.39893047\n",
      "OBJ: 1322.39800532\n",
      "OBJ: 1322.39366213\n",
      "OBJ: 1322.39118794\n",
      "OBJ: 1322.38357327\n",
      "OBJ: 1322.38170217\n",
      "OBJ: 1322.38029015\n",
      "OBJ: 1322.37925678\n",
      "OBJ: 1322.37861235\n",
      "OBJ: 1322.37831756\n",
      "OBJ: 1322.3765122\n",
      "OBJ: 1322.37625732\n",
      "OBJ: 1322.37494308\n",
      "OBJ: 1322.37463309\n",
      "OBJ: 1322.37311875\n",
      "OBJ: 1322.37226694\n",
      "OBJ: 1322.37088263\n",
      "OBJ: 1322.37075436\n",
      "OBJ: 1322.36993039\n",
      "OBJ: 1322.36983511\n",
      "OBJ: 1322.36967936\n",
      "OBJ: 1322.36465039\n",
      "OBJ: 1322.36413032\n",
      "OBJ: 1322.35985656\n",
      "OBJ: 1322.3533559\n",
      "OBJ: 1322.34759664\n",
      "OBJ: 1322.34664665\n",
      "OBJ: 1322.34254536\n",
      "OBJ: 1322.34207238\n",
      "OBJ: 1322.34191199\n",
      "OBJ: 1322.34069246\n",
      "OBJ: 1322.33932346\n",
      "OBJ: 1322.33533637\n",
      "OBJ: 1322.32976026\n",
      "OBJ: 1322.32919267\n",
      "OBJ: 1322.32836349\n",
      "OBJ: 1322.32831469\n",
      "OBJ: 1322.32828031\n",
      "OBJ: 1322.32732575\n",
      "OBJ: 1322.3235506\n",
      "OBJ: 1322.31864726\n",
      "OBJ: 1322.31730193\n",
      "OBJ: 1322.31129298\n",
      "OBJ: 1322.30505753\n",
      "OBJ: 1322.29423052\n",
      "OBJ: 1322.29308634\n",
      "OBJ: 1322.28670183\n",
      "OBJ: 1322.2807984\n",
      "OBJ: 1322.2721313\n",
      "OBJ: 1322.26918737\n",
      "OBJ: 1322.26873508\n",
      "OBJ: 1322.26338435\n",
      "OBJ: 1322.25737381\n",
      "OBJ: 1322.24405947\n",
      "OBJ: 1322.22107119\n",
      "OBJ: 1322.14813945\n",
      "OBJ: 1322.11263927\n",
      "OBJ: 1322.10593539\n",
      "OBJ: 1322.08104537\n",
      "OBJ: 1322.06450395\n",
      "OBJ: 1322.06053585\n",
      "OBJ: 1322.05190302\n",
      "OBJ: 1322.04910583\n",
      "OBJ: 1322.03174428\n",
      "OBJ: 1322.01345403\n",
      "OBJ: 1322.01138916\n",
      "OBJ: 1322.01099341\n",
      "OBJ: 1322.00903245\n",
      "OBJ: 1322.00865849\n",
      "OBJ: 1322.00813731\n",
      "OBJ: 1322.0073182\n",
      "OBJ: 1322.00678385\n",
      "OBJ: 1321.99743262\n",
      "OBJ: 1321.99416888\n",
      "OBJ: 1321.98297942\n",
      "OBJ: 1321.96231349\n",
      "OBJ: 1321.96009845\n",
      "OBJ: 1321.94433049\n",
      "OBJ: 1321.94058204\n",
      "OBJ: 1321.89923003\n",
      "OBJ: 1321.86650941\n",
      "OBJ: 1321.83856055\n",
      "OBJ: 1321.69164161\n",
      "OBJ: 1321.67268278\n",
      "OBJ: 1321.61651164\n",
      "OBJ: 1321.58756074\n",
      "OBJ: 1321.57970171\n",
      "OBJ: 1321.54457797\n",
      "OBJ: 1321.49992902\n",
      "OBJ: 1321.48766866\n",
      "OBJ: 1321.47269307\n",
      "OBJ: 1321.46910701\n",
      "OBJ: 1321.45585144\n",
      "OBJ: 1321.45284928\n",
      "OBJ: 1321.44706065\n",
      "OBJ: 1321.44668219\n",
      "OBJ: 1321.43319575\n",
      "OBJ: 1321.4284113\n",
      "OBJ: 1321.41408953\n",
      "OBJ: 1321.40149688\n",
      "OBJ: 1321.38519681\n",
      "OBJ: 1321.34980755\n",
      "OBJ: 1321.34152433\n",
      "OBJ: 1321.33466562\n",
      "OBJ: 1321.27721187\n",
      "OBJ: 1321.27322246\n",
      "OBJ: 1321.24076135\n",
      "OBJ: 1321.22469021\n",
      "OBJ: 1321.21759598\n",
      "OBJ: 1321.20352204\n",
      "OBJ: 1321.19899888\n",
      "OBJ: 1321.18766208\n",
      "OBJ: 1321.17029151\n",
      "OBJ: 1321.16386334\n",
      "OBJ: 1321.13195904\n",
      "OBJ: 1321.12749073\n",
      "OBJ: 1321.11788047\n",
      "OBJ: 1321.11731691\n",
      "OBJ: 1321.10234515\n",
      "OBJ: 1321.08312449\n",
      "OBJ: 1321.07803485\n",
      "OBJ: 1321.04847639\n",
      "OBJ: 1321.01277413\n",
      "OBJ: 1320.99980315\n",
      "OBJ: 1320.99729946\n",
      "OBJ: 1320.96325796\n",
      "OBJ: 1320.9301959\n",
      "OBJ: 1320.92306215\n",
      "OBJ: 1320.89547861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJ: 1320.89255527\n",
      "OBJ: 1320.89213571\n",
      "OBJ: 1320.88192528\n",
      "OBJ: 1320.86482873\n",
      "OBJ: 1320.86293309\n",
      "OBJ: 1320.85814296\n",
      "OBJ: 1320.85314478\n",
      "OBJ: 1320.85265751\n",
      "OBJ: 1320.85061605\n",
      "OBJ: 1320.84877754\n",
      "OBJ: 1320.84855476\n",
      "OBJ: 1320.84670271\n",
      "OBJ: 1320.84635123\n",
      "OBJ: 1320.84378348\n",
      "OBJ: 1320.84192686\n",
      "OBJ: 1320.84010123\n",
      "OBJ: 1320.83622384\n",
      "OBJ: 1320.83600326\n",
      "OBJ: 1320.83482965\n",
      "OBJ: 1320.83450603\n",
      "OBJ: 1320.83328264\n",
      "OBJ: 1320.83303178\n",
      "OBJ: 1320.83240912\n",
      "OBJ: 1320.83148382\n",
      "OBJ: 1320.83131408\n",
      "OBJ: 1320.83044056\n",
      "OBJ: 1320.83021657\n",
      "OBJ: 1320.83016011\n",
      "OBJ: 1320.83000941\n",
      "OBJ: 1320.82968101\n",
      "OBJ: 1320.82964885\n",
      "OBJ: 1320.82952953\n",
      "OBJ: 1320.82948082\n",
      "OBJ: 1320.82913917\n",
      "OBJ: 1320.82911549\n",
      "OBJ: 1320.82883886\n",
      "OBJ: 1320.82876349\n",
      "OBJ: 1320.82849266\n",
      "OBJ: 1320.82847404\n",
      "OBJ: 1320.82778886\n",
      "OBJ: 1320.82741506\n",
      "OBJ: 1320.82734702\n",
      "OBJ: 1320.82651824\n",
      "OBJ: 1320.82547729\n",
      "OBJ: 1320.82511163\n",
      "OBJ: 1320.82424565\n",
      "OBJ: 1320.82385096\n",
      "OBJ: 1320.82376801\n",
      "OBJ: 1320.82365407\n",
      "OBJ: 1320.82345268\n",
      "OBJ: 1320.82336215\n",
      "OBJ: 1320.82278978\n",
      "OBJ: 1320.82266014\n",
      "OBJ: 1320.82248901\n",
      "OBJ: 1320.82244193\n",
      "OBJ: 1320.82229956\n",
      "OBJ: 1320.82221531\n",
      "OBJ: 1320.82218301\n",
      "OBJ: 1320.82217361\n",
      "OBJ: 1320.82214934\n",
      "OBJ: 1320.82213737\n",
      "OBJ: 1320.8220668\n",
      "OBJ: 1320.82206256\n",
      "OBJ: 1320.82205468\n",
      "OBJ: 1320.82198073\n",
      "OBJ: 1320.82193358\n",
      "OBJ: 1320.82189468\n",
      "OBJ: 1320.8218821\n",
      "OBJ: 1320.82186453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maryam/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:222: UserWarning: Max. number of function evaluations reached\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3cfef4662e02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                                     solver='TNC')\n\u001b[1;32m     21\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mordinal_logistic_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR (ORDINAL)  fold %s: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "X -= X.mean()\n",
    "y -= y.min()\n",
    "\n",
    "idx = np.argsort(y)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "cv = cross_validation.ShuffleSplit(y.size, n_iter=50, test_size=.1, random_state=0)\n",
    "score_logistic = []\n",
    "score_ordinal_logistic = []\n",
    "score_ridge = []\n",
    "for i, (train, test) in enumerate(cv):\n",
    "    #test = train\n",
    "    if not np.all(np.unique(y[train]) == np.unique(y)):\n",
    "        # we need the train set to have all different classes\n",
    "        continue\n",
    "    assert np.all(np.unique(y[train]) == np.unique(y))\n",
    "    train = np.sort(train)\n",
    "    test = np.sort(test)\n",
    "    w, theta = ordinal_logistic_fit(X[train], y[train], verbose=True,\n",
    "                                    solver='TNC')\n",
    "    pred = ordinal_logistic_predict(w, theta, X[test])\n",
    "    1/0\n",
    "    s = metrics.mean_absolute_error(y[test], pred)\n",
    "    print('ERROR (ORDINAL)  fold %s: %s' % (i+1, s))\n",
    "    score_ordinal_logistic.append(s)\n",
    "\n",
    "    from sklearn import linear_model\n",
    "    clf = linear_model.LogisticRegression(C=1.)\n",
    "    clf.fit(X[train], y[train])\n",
    "    pred = clf.predict(X[test])\n",
    "    s = metrics.mean_absolute_error(y[test], pred)\n",
    "    print('ERROR (LOGISTIC) fold %s: %s' % (i+1, s))\n",
    "    score_logistic.append(s)\n",
    "\n",
    "    from sklearn import linear_model\n",
    "    clf = linear_model.Ridge(alpha=1.)\n",
    "    clf.fit(X[train], y[train])\n",
    "    pred = np.round(clf.predict(X[test]))\n",
    "    s = metrics.mean_absolute_error(y[test], pred)\n",
    "    print('ERROR (RIDGE)    fold %s: %s' % (i+1, s))\n",
    "    score_ridge.append(s)\n",
    "\n",
    "\n",
    "print()\n",
    "print('MEAN ABSOLUTE ERROR (ORDINAL LOGISTIC):    %s' % np.mean(score_ordinal_logistic))\n",
    "print('MEAN ABSOLUTE ERROR (LOGISTIC REGRESSION): %s' % np.mean(score_logistic))\n",
    "print('MEAN ABSOLUTE ERROR (RIDGE REGRESSION):    %s' % np.mean(score_ridge))\n",
    "# print('Chance level is at %s' % (1. / np.unique(y).size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
